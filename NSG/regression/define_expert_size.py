import paths
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from NSG import data_processing_methods as dpm

"""
MODEL VALIDATION
----------------
Glass experts have identified a region containing data that have been 
genrated by the melting process.
From the glass experts, we know that:

- Increases in fault density that last between 2 hours and 4 days are most
  likely generated by the melting process.
- Area of interest ranges from: 2020-08-15 to: 2020-08-30
- Furnace faults increses identified at:
    1) From 2020-08-20 to 2020-08-23
    2) From 2020-08-24 to 2020-08-27
"""

"""
NSG data
"""
# NSG post processes data location
file = paths.get_data_path('NSG_data.xlsx')

# Training df
X_df = pd.read_excel(file, sheet_name='X_training_stand')
y_df = pd.read_excel(file, sheet_name='y_training')
y_raw_df = pd.read_excel(file, sheet_name='y_raw_training')
timelags_df = pd.read_excel(file, sheet_name='time')

# Pre-Process training data
X, y0, N0, D, max_lag, time_lags = dpm.align_arrays(X_df, y_df, timelags_df)

# Replace zero values with interpolation
zeros = y_raw_df.loc[y_raw_df['raw_furnace_faults'] < 1e-2]
y_raw_df['raw_furnace_faults'][zeros.index] = None
y_raw_df.interpolate(inplace=True)

# Process raw targets
# Just removes the first max_lag points from the date_time array.
y_raw = dpm.adjust_time_lag(y_raw_df['raw_furnace_faults'].values,
                            shift=0,
                            to_remove=max_lag)

# Extract corresponding time stamps. Note this essentially just
# removes the first max_lag points from the date_time array.
date_time = dpm.adjust_time_lag(y_df['Time stamp'].values,
                                shift=0,
                                to_remove=max_lag)

# Train and test data
N, D = np.shape(X)
# To ensure the first 2 esperts got the first continous region
start_train = 0
end_test = N
N_train = int(N*0.82)

X_train, y_train = X[start_train:N_train], y_raw[start_train:N_train]
X_test, y_test = X[start_train:end_test], y_raw[start_train:end_test]

date_time = date_time[start_train:end_test]
y_raw = y_raw[start_train:end_test]
y_rect = y0[start_train:end_test]

# temp drop noisy region
start_drop = y_df[y_df['Time stamp'] == '2020-08-30'].index[0]
end_drop = y_df[y_df['Time stamp'] == '2020-09-09'].index[0]
remove = np.arange(start_drop, end_drop)

print(np.shape(X_train))
# print(np.arange(start_drop, end_drop))

X_train = np.delete(X_train, remove, 0)
y_train = np.delete(y_train, remove, 0)
X_test = np.delete(X_test, remove, 0)
y_test = np.delete(y_test, remove, 0)
date_time = np.delete(date_time, remove, 0)
y_raw = np.delete(y_raw, remove, 0)
y_rect = np.delete(y_rect, remove, 0)

N_gps = 21
step = int(len(X_train)/N_gps)
print('N-gps: ', N_gps, ' step: ', step, ' N: ',len(y_raw), '22xstep: ', N_gps*step,
      'remove len: ', len(remove))

"""
PLOT
"""

# Region where test data is similar to the training data
similar = range(21000-len(remove),21500-len(remove))
# similar = range(21500, N)

fig, ax = plt.subplots()
fig.autofmt_xdate()
ax.plot(date_time, y_raw, color='grey', label='Raw')
plt.fill_between(date_time[:N_train], 50, color='lightgreen', alpha=0.6,
                 label='training')
plt.fill_between(date_time[similar], 50, color='lightgreen', alpha=0.6,
                 label='test data similar to training')
for k in range(N_gps):
    plt.axvline(date_time[int(k*step)], linestyle='--', linewidth=2,
                color='black', label='div '+str(k))

ax.set_title('Predictive contribution of robust GP experts')
ax.set_xlabel('Date-time')
ax.set_ylabel('Predictive contribution')
# plt.legend(loc=0, prop={"size":18}, facecolor="white", framealpha=1.0)

print('N-train: ', N_train, ' N: ', N, ' N-test: ', int(N - N_train),' step: ', step, ' sparse-step: ', step/2)

# # ----------------------------------------------------------------------------
# # PCA and PLOTS
# # ----------------------------------------------------------------------------
# from sklearn.decomposition import PCA

# pca = PCA(n_components=2)
# pca.fit(X)
# Xt = pca.transform(X)

# # PCA on training data
# Xt_train = pca.transform(X_train)

# # PCA on test data
# Xt_test = pca.transform(X_test)
    
# # Plot at each 1000 points
# step
# fig, ax = plt.subplots()
# ax.plot(Xt[:, 0], Xt[:, 1], 'o', markersize=0.9, c='grey',
#         label='Available training data', alpha=0.9)
# ax.plot(Xt_train[:, 0], Xt_train[:, 1], 'o', markersize=8.9, c='orange',
#         label='Used Training data', alpha=0.6)
# ax.plot(Xt_test[:,0], Xt_test[:,1], '*', markersize=5.5,
#         c='purple', label='test data', alpha=0.6)
# ax.plot(Xt_test[similar,0], Xt_test[similar,1], '*', markersize=5.5,
#         c='limegreen', label='similar', alpha=0.6)
# ax.set_xlim(np.min(Xt[:, 0]), np.max(np.max(Xt[:, 0])))
# ax.set_ylim(np.min(Xt[:, 0]), np.max(np.max(Xt[:, 1])))
plt.show()